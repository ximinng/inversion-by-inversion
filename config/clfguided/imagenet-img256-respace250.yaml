seed: 42
num_samples: 25
# data
dataset: "imagenet-1k"
num_classes: 1000
image_size: 256
channels: 3
# optimizer
lr: 2e-5
adam_betas: [ 0.9, 0.999 ]
weight_decay: 0
# train:
train_num_steps: 700000
train_batch_size: 16
noised: True
schedule_sampler: "uniform"
# sample
sample_batch_size: 5
# params
classifier_scale: 1.0

model:
  type: "ADM"
  use_fp16: True
  load_share_weights: False # use openai ckpt as init
  in_channels: 3
  num_channels: 256
  out_channels: 3
  num_res_blocks: 2
  class_cond: True # involved label embedding, as well as `y`
  use_checkpoint: False
  attention_resolutions: "32,16,8"
  num_heads: 4
  num_head_channels: 64
  num_heads_upsample: -1
  use_scale_shift_norm: True
  dropout: 0
  resblock_updown: True
  use_new_attention_order: False

diffusion:
  beta_schedule: "linear"
  beta_start: 0.0001
  beta_end: 0.02
  timesteps: 1000
  clip_denoised: True
  learn_sigma: True # out_channels * 2
  sigma_small: False
  use_kl: False
  predict_xstart: False
  rescale_timesteps: False
  rescale_learned_sigmas: False
  use_ddim: True
  timestep_respacing: "ddim25" #"250"

classifier:
  use_fp16: False
  load_share_weights: False # use openai ckpt as init
  in_channels: ${channels}
  out_channels: ${num_classes}
  model_channels: 128 # width
  num_res_blocks: 2 # depth
  attention_resolutions: "32,16,8"
  dropout: 0
  num_head_channels: 64
  use_scale_shift_norm: True
  resblock_updown: True
  use_new_attention_order: False
  pool: "attention"